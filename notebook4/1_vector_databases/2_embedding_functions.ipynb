{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding functions\n",
    "\n",
    "Using the appropriate model, anything can be embedded. For this exercise we're going to use embeddings to do semantic search over Rick and Morty quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT doesn't have parents or emotions\n",
    "\n",
    "Vector search is useful for retrieving data that's not part of the model's training data.\n",
    "\n",
    "For example, if we asked the following question to ChatGPT, we get some generic sounding answer wrapping around the core \"she does not make any statements about causing her parents misery\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "Now let's read the quotes from the included text file (source: https://parade.com/tv/rick-and-morty-quotes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_quotes() -> list[str]:\n",
    "    with open(\"rick_and_morty_quotes.txt\", \"r\") as fh:\n",
    "        return fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rick_and_morty_quotes = read_quotes()\n",
    "rick_and_morty_quotes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "emb1, emb2 = model.encode([\n",
    "    \"Losers look stuff up while the rest of us are carpin' all them diems.\\n\",\n",
    "    \"Losers look stuff up while the rest of us are carpin' all them diems.\"\n",
    "])\n",
    "np.allclose(emb1, emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write function to generate embeddings from text\n",
    "\n",
    "Write a function that turns text into embeddings using Sentence Transformers.\n",
    "\n",
    "**HINT**\n",
    "1. Choose a [pre-trained model](https://www.sbert.net/docs/pretrained_models.html), you don't need to create your own\n",
    "2. See the API documentation and examples for Sentence Transformers to see how to encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Union\n",
    "\n",
    "MODEL_NAME = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "def generate_embeddings(input_data: Union[str, list[str]]) -> np.ndarray:    \n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(input_data)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(rick_and_morty_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the embeddings\n",
    "for sentence, embedding in zip(rick_and_morty_quotes[:3], embeddings[:3]):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions is each embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 384次元のベクトル\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行が58, 列が384\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the embeddings normalized already?\n",
    "-> No, they are not. We may want to add a step in the embedding function to normalize all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=1なので行方向のノルムの計算\n",
    "np.linalg.norm(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put it all together\n",
    "\n",
    "First let's encode the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Are you the cause of your parents' misery?\"\n",
    "query_embedding = model.encode(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reuse the find_nearest_neighbors function we wrote for exercise 1.\n",
    "\n",
    "However, that only returns the vectors, whereas we also want the quotes. So please rewrite the find_nearest_neighbors function to return the *indices* of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v1 : np.ndarray\n",
    "        First vector.\n",
    "    v2 : np.ndarray\n",
    "        Second vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Euclidean distance between `v1` and `v2`.\n",
    "    \"\"\"\n",
    "    dist = v1 - v2\n",
    "    return np.linalg.norm(dist, axis=len(dist.shape)-1)\n",
    "\n",
    "\n",
    "def find_nearest_neighbors(query: np.ndarray,\n",
    "                           vectors: np.ndarray,\n",
    "                           k: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find k-nearest neighbors of a query vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : np.ndarray\n",
    "        Query vector.\n",
    "    vectors : np.ndarray\n",
    "        Vectors to search.\n",
    "    k : int, optional\n",
    "        Number of nearest neighbors to return, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The indices of the `k` nearest neighbors of `query` in `vectors`.\n",
    "    \"\"\"\n",
    "    distances = euclidean_distance(query, vectors)\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = find_nearest_neighbors(query_embedding, embeddings, k=3)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices:\n",
    "    print(rick_and_morty_quotes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking the question again\n",
    "\n",
    "Now let's use the retrieved quotes and ask ChatGPT to answer the question based on the quotes in addition to its own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer the question based on the context.\n",
    "\n",
    "Question: Are you the cause of your parents' misery?\n",
    "\n",
    "Context:\n",
    "\n",
    "You're not the cause of your parents' misery. You're just a symptom of it.\n",
    "\n",
    "Having a family doesn't mean that you stop being an individual. You know the best thing you can do for the people that depend on you? Be honest with them, even if it means setting them free.\n",
    "\n",
    "B—h, my generation gets traumatized for breakfast.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
